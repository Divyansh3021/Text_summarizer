{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m PegasusForConditionalGeneration, PegasusTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Text summarization aims at generating accurate and concise\n",
    "summaries from input document(s). In contrast to extractive\n",
    "summarization which merely copies informative fragments\n",
    "from the input, abstractive summarization may generate\n",
    "novel words. A good abstractive summary covers principal\n",
    "information in the input and is linguistically fluent.\n",
    "In abstractive summarization, sequence-to-sequence\n",
    "(Sutskever et al., 2014) has become a dominant framework\n",
    "using encoder-decoder architectures based on RNNs\n",
    "(Chung et al., 2014; Hochreiter & Schmidhuber, 1997)\n",
    "and more recently Transformers (Vaswani et al., 2017).\n",
    "Most prior work on neural abstractive summarization\n",
    "relied on large-scale, high-quality datasets of supervised\n",
    "document-summary pairs (Hermann et al., 2015) and\n",
    "achieved promising results (Rush et al., 2015; Nallapati\n",
    "et al., 2016; See et al., 2017). In recent years, there has\n",
    "been increased interest in collecting new summarization\n",
    "datasets that have more abstractive summaries (Narayan\n",
    "et al., 2018), have longer documents, (Cohan et al., 2018;\n",
    "Sharma et al., 2019), utilize multiple documents (Fabbri\n",
    "et al., 2019), and are sourced from diverse domains (Grusky\n",
    "et al., 2018; Koupaee & Wang, 2018; Kim et al., 2019;\n",
    "Kornilova & Eidelman, 2019; Zhang & Tetreault, 2019);\n",
    "arXiv:1912.08777v3 [cs.CL] 10 Jul 2020\n",
    "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\n",
    "however, there has been little work on systematic evaluation\n",
    "of models across these broad settings.\n",
    "Contemporaneously, the adoption of Transformer models\n",
    "(Vaswani et al., 2017) pre-trained using self-supervised objectives on large text corpora (Radford et al., 2018a; Devlin\n",
    "et al., 2019) have improved performance on many NLP tasks\n",
    "(Wang et al., 2018; Rajpurkar et al., 2016).\n",
    "Recent work leveraging such pre-training for Transformerbased sequence-to-sequence models (Dong et al., 2019;\n",
    "Song et al., 2019; Rothe et al., 2019; Lewis et al., 2019;\n",
    "Raffel et al., 2019) has extended the success to text generation, including abstractive summarization.\n",
    "In this work, we study pre-training objectives specifically\n",
    "for abstractive text summarization and evaluate on 12 downstream datasets spanning news (Hermann et al., 2015;\n",
    "Narayan et al., 2018; Grusky et al., 2018; Rush et al., 2015;\n",
    "Fabbri et al., 2019), science (Cohan et al., 2018), short\n",
    "stories (Kim et al., 2019), instructions (Koupaee & Wang,\n",
    "2018), emails (Zhang & Tetreault, 2019), patents (Sharma\n",
    "et al., 2019), and legislative bills (Kornilova & Eidelman,\n",
    "2019). We find that masking whole sentences from a document and generating these gap-sentences from the rest of the\n",
    "document works well as a pre-training objective for downstream summarization tasks. In particular, choosing putatively important sentences outperforms lead or randomly\n",
    "selected ones. We hypothesize this objective is suitable for\n",
    "abstractive summarization as it closely resembles the downstream task, encouraging whole-document understanding\n",
    "and summary-like generation. We call this self-supervised\n",
    "objective Gap Sentences Generation (GSG). Using GSG\n",
    "to pre-train a Transformer encoder-decoder on large corpora of documents (Web and news articles) results in our\n",
    "method, Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence models, or\n",
    "PEGASUS.\n",
    "With our best 568M parameter model trained on the recently introduced C4 (Raffel et al., 2019) corpus we equal\n",
    "or exceed state-of-the-art on the 12 summarization tasks\n",
    "we consider. We further push forward the state-of-the-art\n",
    "using a newly collected text corpus comprised of news-like\n",
    "articles we call HugeNews, including the highly competitive\n",
    "XSum and CNN/DailyMail summarization datasets.\n",
    "Large-scale document-summary datasets are rare and in\n",
    "practice there is a mismatch between research datasets and\n",
    "real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our\n",
    "model is able to adapt very quickly when fine-tuning with\n",
    "small numbers of supervised pairs, obtaining state-of-the-art\n",
    "results in 6 datasets with only 1000 examples.\n",
    "Qualitatively we observed high quality outputs from our\n",
    "best models and validated this in human evaluation studies.\n",
    "We found that PEGASUS summaries are at least as good as\n",
    "reference summaries for the datasets we assessed – XSum,\n",
    "CNN/DailyMail, and Reddit TIFU – even at low-levels of\n",
    "supervision.\n",
    "To summarize our contributions:\n",
    "• We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences.\n",
    "• We evaluate the proposed pre-training objective on a\n",
    "broad range of downstream summarization tasks, with\n",
    "careful ablations to choose the best model settings,\n",
    "which we use to train a 568M parameter PEGASUS\n",
    "model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered.\n",
    "• We show how good abstractive summarization performance can be achieved across broad domains with\n",
    "very little supervision by fine-tuning the PEGASUS\n",
    "model and surpassing previous state-of-the-art results\n",
    "on many tasks with as little as 1000 examples.\n",
    "• We conducted human evaluation studies to validate our\n",
    "experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail,\n",
    "and Reddit TIFU.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, truncation = True, max_length=500, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 8543,  5906,  6520,  3884,  3921,   134,  7288,  2598,   111, 14710,\n",
       "         27533,   135,  3196,  2199,   741,   116,   250,   222,  3945,   112,\n",
       "         73840,  5906,  6520,  3884,   162,  5127,  4862,  7096, 19320,   135,\n",
       "           109,  3196,   108,  7093,  5551,  5906,  6520,  3884,   218,  3210,\n",
       "          2794,   989,   107,   202,   234,  7093,  5551,  5627,  2370,  5191,\n",
       "           257,   115,   109,  3196,   111,   117, 92366, 21956,   107,   222,\n",
       "          7093,  5551,  5906,  6520,  3884,   108,  5936,   121,   497,   121,\n",
       "         69987,   143, 20159,   144, 11384,  9433,  3256,  2700,   107,   108,\n",
       "         36473,   148,   460,   114,  9574,  3772,   303, 40753,   121,  2534,\n",
       "         56636, 31576,   451,   124,   840, 21323,   116,   143, 82159,  3256,\n",
       "          2700,   107,   108, 48464, 47570,   216, 24178,   259, 73386, 23503,\n",
       "           420,   108, 58986,   111,   154,   938, 38979,   143, 20245,   116,\n",
       "         35668,  3256,  2700,   107,   108, 35436,  1386,  1620,   201,   124,\n",
       "         14849,  7093,  5551,  5906,  6520,  3884, 13839,   124,   423,   121,\n",
       "          5129,   108,   281,   121,  3126, 24089,   113, 15561,  2199,   121,\n",
       "         94307,  8052,   143,  6593,  8427,  3256,  2700,   107,   108, 31722,\n",
       "           111,  3310,  7367,   602,   143, 52867,  3256,  2700,   107,   108,\n",
       "         44793,  1101, 24169, 25265,  3256,  2700,   107,   108, 51514,  1883,\n",
       "          3256,  2700,   107,   108, 35436,   222,   909,   231,   108,   186,\n",
       "           148,   174,  1562,   820,   115,  6073,   177,  5906,  6520,  3884,\n",
       "         24089,   120,   133,   154,  7093,  5551, 27533,   143, 51729, 46721,\n",
       "          3256,  2700,   107,   108, 28505,   108,   133,   895,  2010,   108,\n",
       "           143,   529, 52583,  3256,  2700,   107,   108,   931,   206, 19368,\n",
       "          3256,  2700,   107,   108,  1231,   312,  4291,  1079,  2010,   143,\n",
       "         36316, 19192,  3256,  2700,   107,   108,  1231,   312,   111,   127,\n",
       "         10460,   135,  2766,  9982,   143, 65069,  8827,  3256,  2700,   107,\n",
       "           108,   931,   206, 28168,  6035,  5747,   259, 12167,   108,   931,\n",
       "           206,  5377,  3256,  2700,   107,   108,  1231,   206, 44437,   457,\n",
       "         40681,   259, 29953, 35156,   108,  1231,   206, 18777,   259, 41586,\n",
       "           216, 34019,   108,  1231,  3760,   114,   551,  1880,  9757,   151,\n",
       "         79798,   107, 10660, 30247,  2075,   726,  1126, 15032,   107, 15323,\n",
       "          1100,   377, 12453,  7149, 49921, 89637,   151,  3414,   121, 18006,\n",
       "           122, 14892,   316, 18306,   121, 70475,   116,   118, 13609,  5551,\n",
       "         92429,   551,  3884,   802,   108,   186,   148,   174,   332,   201,\n",
       "           124, 11624,  4051,   113,  1581,   482,   219,  3426,  2499,   107,\n",
       "         80920, 73111,   445,   108,   109,  5326,   113, 51979,  1581,   143,\n",
       "         20245,   116, 35668,  3256,  2700,   107,   108, 30175,  1133,   121,\n",
       "         14787,   303,   813,   121, 83465,  4358,   124,   423,  1352,   110,\n",
       "         88758,   143, 46517,  4759,  3256,  2700,   107,   108,   931,   304,\n",
       "           206, 82642,  3256,  2700,   107,   108, 45294,   133,  2521,   637,\n",
       "           124,   223, 36789,  2722,   143, 54297,  3256,  2700,   107,   108,\n",
       "           931,   206, 16117,  8753, 10310,  3256,  2700,   107,   108, 38128,\n",
       "         13618,   201, 15479,   253,  1133,   121, 18006,   118, 51979,   936,\n",
       "          5936,   121,   497,   121, 69987,  1581,   143, 75808,  3256,  2700,\n",
       "           107,   108,  1231,   206,  7691,  3256,  2700,   107,   108,  1231,\n",
       "           206, 14390,   326,  3256,  2700,   107,   108,  1231,   206,  6423,\n",
       "          3256,  2700,   107,   108,  1231,   206, 55139,  2734,  3256,  2700,\n",
       "           107,   108, 45294,   148,  3038,   109,   924,   112,  1352,  2233,\n",
       "           108,   330,  7093,  5551,  5906,  6520,  3884,   107,   222,   136,\n",
       "           201,   108,   145,   692,  1133,   121, 18006,  4358,  2304,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   222,   136,   201,   108,   145,   692,  1133,   121, 18006,\n",
       "          4358,  2304,   118,  7093,  5551,  5906,  6520,  3884,   107,     1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>In this work, we study pre-training objectives specifically for abstractive summarization.</s>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(summary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cfccc6f142da531e1d4135378bb56fa9565344159fa8b3b34b0ee44f826689f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
